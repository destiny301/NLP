{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lingo-mit/6864-hw1/blob/master/6864_hw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FU7xWiY6TyWS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3J\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
    "rm -rf 6864-hw1\n",
    "git clone https://github.com/lingo-mit/6864-hw1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0MHaHrdUACZ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab_util'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-52551a8e279f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlab_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lab_util'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/content/6864-hw1\")\n",
    "\n",
    "import csv\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import lab_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZ3MUj4iUf76"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this lab, you'll explore three different ways of using unlabeled text data to learn pretrained word representations. Your lab report will describe the effects of different modeling decisions (representation learning objective, context size, etc.) on both qualitative properties of learned representations and their effect on a downstream prediction problem.\n",
    "\n",
    "**General lab report guidelines**\n",
    "\n",
    "Homework assignments should be submitted in the form of a research report. (We'll be providing a place to upload them before the due date, but are still sorting out some logistics.) Please upload PDFs, with a maximum of four single-spaced pages. (If you want you can use the [Association for Computational Linguistics style files](http://acl2020.org/downloads/acl2020-templates.zip).) Reports should have one section for each part of the homework assignment below. Each section should describe the details of your code implementation, and include whatever charts / tables are necessary to answer the set of questions at the end of the corresponding homework part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gG654Y9J3yHw"
   },
   "source": [
    "\n",
    "We're going to be working with a dataset of product reviews. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JwiX-Tc9V1xI"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "n_positive = 0\n",
    "n_disp = 0\n",
    "with open(\"/content/6864-hw1/reviews.csv\") as reader:\n",
    "  csvreader = csv.reader(reader)\n",
    "  next(csvreader)\n",
    "  for id, review, label in csvreader:\n",
    "    label = int(label)\n",
    "\n",
    "    # hacky class balancing\n",
    "    if label == 1:\n",
    "      if n_positive == 2000:\n",
    "        continue\n",
    "      n_positive += 1\n",
    "    if len(data) == 4000:\n",
    "      break\n",
    "\n",
    "    data.append((review, label))\n",
    "    \n",
    "    if n_disp > 5:\n",
    "      continue\n",
    "    n_disp += 1\n",
    "    print(\"review:\", review)\n",
    "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
    "    print()\n",
    "\n",
    "print(f\"Read {len(data)} total reviews.\")\n",
    "np.random.shuffle(data)\n",
    "reviews, labels = zip(*data)\n",
    "train_reviews = reviews[:3000]\n",
    "train_labels = labels[:3000]\n",
    "val_reviews = reviews[3000:3500]\n",
    "val_labels = labels[3000:3500]\n",
    "test_reviews = reviews[3500:]\n",
    "test_labels = labels[3500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8h4k40QoXFO5"
   },
   "source": [
    "We've provided a little bit of helper code for reading in the dataset; your job is to implement the learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "twLHWqM6Z5xD"
   },
   "source": [
    "## Part 1: word representations via matrix factorization\n",
    "\n",
    "First, we'll construct the term--document matrix (look at `/content/6864-hw1/lab_util.py` in the file browser on the left if you want to see how this works)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3WPt6Y7-Z_7P"
   },
   "outputs": [],
   "source": [
    "vectorizer = lab_util.CountVectorizer()\n",
    "vectorizer.fit(train_reviews)\n",
    "td_matrix = vectorizer.transform(train_reviews).T\n",
    "print(f\"TD matrix is {td_matrix.shape[0]} x {td_matrix.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hd3-pw4XbD4B"
   },
   "source": [
    "First, implement a function that computes word representations via latent semantic analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KASVs8KubeBE"
   },
   "outputs": [],
   "source": [
    "def learn_reps_lsa(matrix, rep_size):\n",
    "  # `matrix` is a `|V| x n` matrix, where `|V|` is the number of words in the\n",
    "  # vocabulary. This function should return a `|V| x rep_size` matrix with each\n",
    "  # row corresponding to a word representation. The `sklearn.decomposition` \n",
    "  # package may be useful.\n",
    "\n",
    "  # Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SKWzRC0dclVK"
   },
   "source": [
    "Let's look at some representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Ad7RZkwceWw"
   },
   "outputs": [],
   "source": [
    "reps = learn_reps_lsa(td_matrix, 500)\n",
    "words = [\"good\", \"bad\", \"cookie\", \"jelly\", \"dog\", \"the\", \"4\"]\n",
    "show_tokens = [vectorizer.tokenizer.word_to_token[word] for word in words]\n",
    "lab_util.show_similar_words(vectorizer.tokenizer, reps, show_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsOAGLB3iRjT"
   },
   "source": [
    "We've been operating on the raw count matrix, but in class we discussed several reweighting schemes aimed at making LSA representations more informative. \n",
    "\n",
    "Here, implement the TF-IDF transform and see how it affects learned representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1y3PmW-IgpqA"
   },
   "outputs": [],
   "source": [
    "def transform_tfidf(matrix):\n",
    "  # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the \n",
    "  # vocabulary size and `|D|` is the number of documents in the corpus. This\n",
    "  # function should (nondestructively) return a version of `matrix` with the\n",
    "  # TF-IDF transform appliied.\n",
    "\n",
    "  # Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xOprgqzHi7bk"
   },
   "source": [
    "How does this change the learned similarity function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SV5xKLYTi7LA"
   },
   "outputs": [],
   "source": [
    "td_matrix_tfidf = transform_tfidf(td_matrix)\n",
    "reps_tfidf = learn_reps_lsa(td_matrix_tfidf, 500)\n",
    "lab_util.show_similar_words(vectorizer.tokenizer, reps_tfidf, show_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HO-NG4u1kG9z"
   },
   "source": [
    "Now that we have some representations, let's see if we can do something useful with them.\n",
    "\n",
    "Below, implement a feature function that represents a document as the sum of its\n",
    "learned word embeddings.\n",
    "\n",
    "The remaining code trains a logistic regression model on a set of *labeled* reviews; we're interested in seeing how much representations learned from *unlabeled* reviews improve classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6B08xvIFlee3"
   },
   "outputs": [],
   "source": [
    "def word_featurizer(xs):\n",
    "  # normalize\n",
    "  return xs / np.sqrt((xs ** 2).sum(axis=1, keepdims=True))\n",
    "\n",
    "def lsa_featurizer(xs):\n",
    "  # This function takes in a matrix in which each row contains the word counts\n",
    "  # for the given review. It should return a matrix in which each row contains\n",
    "  # the learned feature representation of each review (e.g. the sum of LSA \n",
    "  # word representations).\n",
    "\n",
    "  feats = None # Your code here!\n",
    "\n",
    "  # normalize\n",
    "  return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
    "\n",
    "def combo_featurizer(xs):\n",
    "  return np.concatenate((word_featurizer(xs), lsa_featurizer(xs)), axis=1)\n",
    "\n",
    "def train_model(featurizer, xs, ys):\n",
    "  import sklearn.linear_model\n",
    "  xs_featurized = featurizer(xs)\n",
    "  model = sklearn.linear_model.LogisticRegression()\n",
    "  model.fit(xs_featurized, ys)\n",
    "  return model\n",
    "\n",
    "def eval_model(model, featurizer, xs, ys):\n",
    "  xs_featurized = featurizer(xs)\n",
    "  pred_ys = model.predict(xs_featurized)\n",
    "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
    "\n",
    "def training_experiment(name, featurizer, n_train):\n",
    "  print(f\"{name} features, {n_train} examples\")\n",
    "  train_xs = vectorizer.transform(train_reviews[:n_train])\n",
    "  train_ys = train_labels[:n_train]\n",
    "  test_xs = vectorizer.transform(test_reviews)\n",
    "  test_ys = test_labels\n",
    "  model = train_model(featurizer, train_xs, train_ys)\n",
    "  eval_model(model, featurizer, test_xs, test_ys)\n",
    "  print()\n",
    "\n",
    "training_experiment(\"word\", word_featurizer, 10)\n",
    "training_experiment(\"lsa\", lsa_featurizer, 10)\n",
    "training_experiment(\"combo\", combo_featurizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rpXziVNrlfp2"
   },
   "source": [
    "**Part 1: Lab writeup**\n",
    "\n",
    "Part 1 of your lab report should discuss any implementation details that were important to filling out the code above. Then, use the code to set up experiments that answer the following questions:\n",
    "\n",
    "1. Qualitatively, what do you observe about nearest neighbors in representation    space? (E.g. what words are most similar to _the_, _dog_, _3_, and _good_?)\n",
    "\n",
    "2. How does the size of the LSA representation affect this behavior?\n",
    "\n",
    "\n",
    "3. Recall that the we can compute the word co-occurrence matrix $W_{tt} = W_    \n",
    "   {td} W_{td}^\\top$. What can you prove about the relationship between the    \n",
    "   left singular vectors of $W_{td}$ and $W_{tt}$? Do you observe this behavior \n",
    "   with your implementation of `learn_reps_lsa`? Why or why not?\n",
    "\n",
    "4. Do learned representations help with the review classification problem? What\n",
    "   is the relationship between the number of labeled examples and the effect of\n",
    "   word embeddings?\n",
    "   \n",
    "5. What is the relationship between the size of the word embeddings and their      usefulness for the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AxfunCYh5nmZ"
   },
   "source": [
    "## Part 2: word representations via language modeling\n",
    "\n",
    "In this section, we'll train a word embedding model with a word2vec-style objective rather than a matrix factorization objective. This requires a little more work; we've provided scaffolding for a PyTorch model implementation below.\n",
    "(If you've never used PyTorch before, there are some tutorials [here](https://pytorch.org/tutorials/). You're also welcome to implement these experiments in\n",
    "any other framework of your choosing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M1napibQ6aub"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "\n",
    "class Word2VecModel(nn.Module):\n",
    "  # A torch module implementing a word2vec predictor. The `forward` function\n",
    "  # should take a batch of context word ids as input and predict the word \n",
    "  # in the middle of the context as output, as in the CBOW model from lecture.\n",
    "\n",
    "  def __init__(self, vocab_size, embed_dim):\n",
    "      super().__init__()\n",
    "\n",
    "      # Your code here!\n",
    "\n",
    "  def forward(self, context):\n",
    "      # Context is an `n_batch x n_context` matrix of integer word ids\n",
    "      # this function should return a set of scores for predicting the word \n",
    "      # in the middle of the context\n",
    "\n",
    "      # Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePgZlityuWr3"
   },
   "outputs": [],
   "source": [
    "def learn_reps_word2vec(corpus, window_size, rep_size, n_epochs, n_batch):\n",
    "  # This method takes in a corpus of training sentences. It returns a matrix of\n",
    "  # word embeddings with the same structure as used in the previous section of \n",
    "  # the assignment. (You can extract this matrix from the parameters of the \n",
    "  # Word2VecModel.)\n",
    "\n",
    "  tokenizer = lab_util.Tokenizer()\n",
    "  tokenizer.fit(corpus)\n",
    "  tokenized_corpus = tokenizer.tokenize(corpus)\n",
    "\n",
    "  ngrams = lab_util.get_ngrams(tokenized_corpus, window_size)\n",
    "\n",
    "  device = torch.device('cuda')  # run on colab gpu\n",
    "  model = Word2VecModel(tokenizer.vocab_size, rep_size).to(device)\n",
    "  opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "  loss_fn = None # Your code here\n",
    "\n",
    "  loader = torch_data.DataLoader(ngrams, batch_size=n_batch, shuffle=True)\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "    for context, label in loader:\n",
    "      # as described above, `context` is a batch of context word ids, and\n",
    "      # `label` is a batch of predicted word labels\n",
    "      pass\n",
    "      # Your code here!\n",
    "\n",
    "  # reminder: you want to return a `vocab_size x embedding_size` numpy array\n",
    "  embedding_matrix = None\n",
    "  # Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaUy1cNuB3W1"
   },
   "outputs": [],
   "source": [
    "reps_word2vec = learn_reps_word2vec(train_reviews, 2, 500, 10, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O3oE-tpR7I39"
   },
   "source": [
    "After training the embeddings, we can try to visualize the embedding space to see if it makes sense. First, we can take any word in the space and check its closest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMW4QND56bHF"
   },
   "outputs": [],
   "source": [
    "lab_util.show_similar_words(vectorizer.tokenizer, reps_word2vec, show_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ue-9CPSc7fi9"
   },
   "source": [
    "We can also cluster the embedding space. Clustering in 4 or more dimensions is hard to visualize, and even clustering in 2 or 3 can be difficult because there are so many words in the vocabulary. One thing we can try to do is assign cluster labels and qualitiatively look for an underlying pattern in the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-Yf6NMCXVx4"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "indices = KMeans(n_clusters=10).fit_predict(reps_word2vec)\n",
    "zipped = list(zip(range(vectorizer.tokenizer.vocab_size), indices))\n",
    "np.random.shuffle(zipped)\n",
    "zipped = zipped[:100]\n",
    "zipped = sorted(zipped, key=lambda x: x[1])\n",
    "for token, cluster_idx in zipped:\n",
    "  word = vectorizer.tokenizer.token_to_word[token]\n",
    "  print(f\"{word}: {cluster_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ci1TkENU78Wn"
   },
   "source": [
    "Finally, we can use the trained word embeddings to construct vector representations of full reviews. One common approach is to simply average all the word embeddings in the review to create an overall embedding. Implement the transform function in Word2VecFeaturizer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A5vjmRV6Dgbu"
   },
   "outputs": [],
   "source": [
    "def lsa_featurizer(xs):\n",
    "  feats = None # Your code here!\n",
    "\n",
    "  # normalize\n",
    "  return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
    "\n",
    "training_experiment(\"word2vec\", lsa_featurizer, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XSfoQbxaXtfH"
   },
   "source": [
    "**Part 2: Lab writeup**\n",
    "\n",
    "Part 2 of your lab report should discuss any implementation details that were important to filling out the code above. Then, use the code to set up experiments that answer the following questions:\n",
    "\n",
    "1. Qualitatively, what do you observe about nearest neighbors in representation space? (E.g. what words are most similar to _the_, _dog_, _3_, and _good_?) How well do word2vec representations correspond to your intuitions about word similarity?\n",
    "\n",
    "2. One important parameter in word2vec-style models is context size. How does changing the context size affect the kinds of representations that are learned?\n",
    "\n",
    "3. How do results on the downstream classification problem compare to \n",
    "   part 1?\n",
    "\n",
    "4. What are some advantages and disadvantages of learned embedding representations, relative to the featurization done in part 1?\n",
    "\n",
    "5. What are some potential problems with constructing a representation of the review by averaging the embeddings of the individual words?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "6864-hw1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
