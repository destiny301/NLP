{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6864_hw1b",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZu3XLyDSbfpOdgh0IV6ip",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lingo-mit/6864-hw1/blob/master/6864_hw1b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N655YeL2eEUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit \n",
        "rm -rf 6864-hw1\n",
        "git clone https://github.com/lingo-mit/6864-hw1.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5R8vijdeKgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/6864-hw1\")\n",
        "\n",
        "import csv\n",
        "import itertools as it\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaoYiysseNIH",
        "colab_type": "text"
      },
      "source": [
        "## Hidden Markov Models\n",
        "\n",
        "In the remaining part of the lab (containing part 3) you'll use the Baum--Welch algorithm to learn _categorical_ representations of words in your vocabulary. Answers to questions in this lab should go in the same report as the initial release.\n",
        "\n",
        "As before, we'll start by loading up a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUn-q_pIeuAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = []\n",
        "n_positive = 0\n",
        "n_disp = 0\n",
        "with open(\"/content/6864-hw1/reviews.csv\") as reader:\n",
        "  csvreader = csv.reader(reader)\n",
        "  next(csvreader)\n",
        "  for id, review, label in csvreader:\n",
        "    label = int(label)\n",
        "\n",
        "    # hacky class balancing\n",
        "    if label == 1:\n",
        "      if n_positive == 2000:\n",
        "        continue\n",
        "      n_positive += 1\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "    \n",
        "    if n_disp > 5:\n",
        "      continue\n",
        "    n_disp += 1\n",
        "    print(\"review:\", review)\n",
        "    print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
        "    print()\n",
        "\n",
        "print(f\"Read {len(data)} total reviews.\")\n",
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "train_reviews = reviews[:3000]\n",
        "train_labels = labels[:3000]\n",
        "val_reviews = reviews[3000:3500]\n",
        "val_labels = labels[3000:3500]\n",
        "test_reviews = reviews[3500:]\n",
        "test_labels = labels[3500:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qlqRHoe3y-",
        "colab_type": "text"
      },
      "source": [
        "Next, implement the forward--backward algorithm for HMMs like we saw in class.\n",
        "\n",
        "**IMPORTANT NOTE**: if you directly multiply probabilities as shown on the class slides, you'll get underflow errors. You'll probably want to work in the log domain (remember that `log(ab) = log(a) + log(b)`, `log(a+b) = logaddexp(a, b)`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wVf4QVIfBdc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hmm model\n",
        "class HMM(object):\n",
        "    def __init__(self, num_states, num_words):\n",
        "        self.num_states = num_states\n",
        "        self.num_words = num_words\n",
        "\n",
        "        self.states = range(num_states)\n",
        "        self.symbols = range(num_words)\n",
        "\n",
        "        # initialize the matrix A with random transition probabilities p(j|i)\n",
        "        # A should be a matrix of size `num_states x num_states`\n",
        "        # with rows that sum to 1\n",
        "        self.A = None # your code here\n",
        "\n",
        "        # initialize the matrix B with random emission probabilities p(o|i)\n",
        "        # B should be a matrix of size `num_states x num_words`\n",
        "        # with rows that sum to 1\n",
        "        self.B = None # your code here\n",
        "\n",
        "        # initialize the vector pi with a random starting distribution\n",
        "        # pi should be a vector of size `num_states`\n",
        "        self.pi = None # your code here\n",
        "\n",
        "    def generate(self, n):\n",
        "        \"\"\"randomly sample the HMM to generate a sequence.\n",
        "        \"\"\"\n",
        "        # we'll give you this one\n",
        "\n",
        "        sequence = []\n",
        "        # initialize the first state\n",
        "        state = np.random.choice(self.states, p=self.pi)\n",
        "        for i in range(n):\n",
        "            # get the emission probs for this state\n",
        "            b = self.B[state, :]\n",
        "            # emit a word\n",
        "            word = np.random.choice(self.symbols, p=b)\n",
        "            sequence.append(word)\n",
        "            # get the transition probs for this state\n",
        "            a = self.A[state, :]\n",
        "            # update the state\n",
        "            state = np.random.choice(self.states, p=a)\n",
        "        return sequence\n",
        "\n",
        "    def forward(self, obs):\n",
        "        # run the forward algorithm\n",
        "        # this function should return a `len(obs) x num_states` matrix\n",
        "        # where the (i, j)th entry contains p(obs[:t], hidden_state_t = i)\n",
        "\n",
        "        alpha = np.zeros(len(obs), self.num_states)\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        return alpha\n",
        "\n",
        "    def backward(self, obs):\n",
        "        # run the backward algorithm\n",
        "        # this function should return a `len(obs) x num_states` matrix\n",
        "        # where the (i, j)th entry contains p(obs[t+1:] | hidden_state_t = i)\n",
        "\n",
        "        beta = np.zeros(len(obs), self.num_states)\n",
        "\n",
        "        # your code here!\n",
        "\n",
        "        return beta\n",
        "        \n",
        "    def forward_backward(self, obs):\n",
        "        # compute forward--backward scores\n",
        "\n",
        "        # logprob is the total log-probability of the sequence obs \n",
        "        # (marginalizing over hidden states)\n",
        "\n",
        "        # gamma is a matrix of size `len(obs) x num_states`\n",
        "        # it contains the marginal probability of being in state i at time t\n",
        "\n",
        "        # xi is a tensor of size `len(obs) x num_states x num_states`\n",
        "        # it conains the marginal probability of transitioning from i to j at t\n",
        "\n",
        "        logprob = None\n",
        "        xi = None\n",
        "        gamma = None\n",
        "        # your code here!\n",
        "\n",
        "        return logprob, xi, gamma\n",
        "\n",
        "    def learn_unsupervised(self, corpus, num_iters):\n",
        "        \"\"\"Run the Baum Welch EM algorithm\n",
        "        \"\"\"\n",
        "\n",
        "        for i_iter in range(num_iters):\n",
        "            expected_si = None # your code here\n",
        "            expected_sij = None # your code here\n",
        "            total_logprob = 0\n",
        "            for review in corpus:\n",
        "                logprob, xi, gamma = self.forward_backward(review)\n",
        "                # your code here \n",
        "            print(\"log-likelihood\", total_logprob)\n",
        "            A_new = None # your code here\n",
        "            B_new = None # your code here\n",
        "\n",
        "            self.A = A_new\n",
        "            self.B = B_new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF-l7WucpCBP",
        "colab_type": "text"
      },
      "source": [
        "Train a model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTWXUt15pDg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = lab_util.Tokenizer()\n",
        "tokenizer.fit(train_reviews)\n",
        "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
        "print(tokenizer.vocab_size)\n",
        "\n",
        "hmm = HMM(num_states=10, num_words=tokenizer.vocab_size)\n",
        "hmm.learn_unsupervised(train_reviews_tk, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiCwE05xqXmI",
        "colab_type": "text"
      },
      "source": [
        "Let's look at some of the words associated with each hidden state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhMoLUFqbn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(hmm.num_states):\n",
        "    most_probable = np.argsort(hmm.B[i, :])[:10]\n",
        "    print(f\"state {i}\")\n",
        "    for o in most_probable:\n",
        "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAQ_PmASwdFz",
        "colab_type": "text"
      },
      "source": [
        "We can also look at some samples from the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj1eT3s3wgFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9Qk9adNr7lQ",
        "colab_type": "text"
      },
      "source": [
        "Finally, let's repeat the classification experiment from Parts 1 and 2, using the _vector of expected hidden state counts_ as a sentence representation.\n",
        "\n",
        "(Warning! results may not be the same as in earlier versions of this experiment.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL6JQXLJspyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(xs_featurized, ys):\n",
        "  import sklearn.linear_model\n",
        "  model = sklearn.linear_model.LogisticRegression()\n",
        "  model.fit(xs_featurized, ys)\n",
        "  return model\n",
        "\n",
        "def eval_model(model, xs_featurized, ys):\n",
        "  pred_ys = model.predict(xs_featurized)\n",
        "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "    print(f\"{name} features, {n_train} examples\")\n",
        "    train_xs = np.array([\n",
        "        hmm_featurizer(tokenizer.tokenize(review)) \n",
        "        for review in train_reviews[:n_train]\n",
        "    ])\n",
        "    train_ys = train_labels[:n_train]\n",
        "    test_xs = np.array([\n",
        "        hmm_featurizer(tokenizer.tokenize(review))\n",
        "        for review in test_reviews\n",
        "    ])\n",
        "    test_ys = test_labels\n",
        "    model = train_model(train_xs, train_ys)\n",
        "    eval_model(model, test_xs, test_ys)\n",
        "    print()\n",
        "\n",
        "def hmm_featurizer(review):\n",
        "    _, _, gamma = hmm.forward_backward(review)\n",
        "    return gamma.sum(axis=0)\n",
        "\n",
        "training_experiment(\"hmm\", hmm_featurizer, n_train=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W0qx41Bu2aF",
        "colab_type": "text"
      },
      "source": [
        "**Part 3: Lab writeup**\n",
        "\n",
        "1. What do the learned hidden states seem to encode when you run unsupervised \n",
        "   HMM training with only 2 states? What about 10? What about 100?\n",
        "\n",
        "2. As before, what's the relationship between # of labeled examples and    \n",
        "   usefulness of HMM-based sentence representations? Are these results generally\n",
        "   better or worse than in Parts 1 and 2 of the homework? Why or why not might\n",
        "   HMM state distributions be sensible sentence representations?"
      ]
    }
  ]
}